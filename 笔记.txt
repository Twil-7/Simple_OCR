1、OCR，Optical Character Recognition，意思是通过光学技术对字符进行识别。OCR的概念产生于1929年，德国科学家Tausheck首先提出了OCR，并且申请了专利。几年后，美国科学家Tausheck也提出了利用光学技术对字符进行识别的想法。但这种梦想直到计算机的诞生才变成现实。现在这一技术已经由计算机实现，OCR就演变成利用光学技术对字符进行扫描、识别并转化成计算机内编码的技术了。


2、我有一个想法：

做车牌号识别时，我苦于没有合适的数据集，我总不可能对每个省都搜集一部分车牌号图片吧...正是因为如此，我crnn模型训练失效，只能手动分割出汉字区域，而这又进一步造成了我汉字识别过程中的分类不稳定......

我突然想到一个idea：干脆不从车牌号识别角度出发，对整个大类字符OCR进行训练，生成许许多多模拟字符串，基于此能得到一个覆盖方方面面的准确模型，这个模型自然能够很轻易的处理我的车牌号识别问题。

模拟字符串的时候，我没必要固定省份在第一个位置，可以全部随机，以保证类别出现的均衡性。


3、甚至基于今天看到的论文，我还有个idea：

干脆我不用裁减图片，直接利用CNN提取整张车牌号图片的特征，然后直接分类成不同省份。
模型会设置好相应的权重1，自动替我忽略掉后面的数字字符信息，这样的话我直接不用分割了！！！

这种处理思路很石破天惊！！！直接基于小权重替换掉分割这一步骤，提升算法稳定性和精度！！！


4、我想到了一个巨nb的发paper思路：针对受限OCR的文本识别问题，借助mask-rcnn和此处CNN的处理思路，在vgg后面引入一个分支，专门来对汉字省份做分类，从而完全避开了裁减问题，可以大大提升视频检测稳定性和算法精度。


5、识别验证码属于OCR任务中的一个典型应用。一般来讲，验证码图片中的文字长度都是固定的，验证码图片的尺寸也是固定的。利用这些规律，可以把识别验证码问题当作一个多分类任务来做处理，即对验证码图片进行特征计算，再对相应位置的特征进行分类计算。

使用captcha库生成验证码共分为两步：
（1）实例化captcha模块的ImageCaptcha类，并指定验证码的尺寸和字体。
（2）调用ImageCaptcha类对象的generate_image方法，传入字符串即可生成验证码。


6、我们要对文字图片中的每个字符进行一次分类处理。卷积网络模型的结构并不是唯一的，但卷积网络模型的目的只有一个，就是降维，要用更低维度的数据最大限度地表示出原始像素的含义。

感悟：我突然想到一个问题，传统方法做车牌号分类时，大家想到的都是先做检测定位，再把一整张图片进行分割，分割成不同的字符小块，然后再分别进行分类。人们全部都潜意识地陷入了一个误区，一定要先分割再做分类吗，我不分割可不可以拿一整张图片去做分类？？？可以，倘若模型设置的权重合理，它就会主动忽略掉后面一大段的feature map，只对特定区域做分类。这样就避免了十分难以处理的分割问题。

啊啊啊啊啊！我甚至可以在论文中生成一张feauture map热力图，可视化对一整张图片分类单独某个区域的合理性。

思考更深一步，我甚至找到了一种可以实现图片信息分割的有效方法。我们依然对整张图片进行信息提取，基于标注好的label进行权重训练，通过较小的权重部分，自然而然的实现图片的分割处理。正好解决了目前我的一个大问题，检测汉字时容易出现跳动，因为裁减不稳定造成分类不稳定，而如果我用上一整张图片的特征，就不会出现这种跳动了！！！



7、多分类输出层的作用是：将验证码输出的每个字符都当作一个分类任务去执行。在该例子中识别出验证码的字符数量是6，因此，需要对卷积提取的特征进行6次分类，每次分类的结果对应一个字符。

模型在经过卷积网络处理之后得到了尺寸为(1, 9)的特征数据，之后用6个相同的网络结构对图片中的每个字符进行分类。该模型能够按照顺序识别出验证码，这就说明输出层的6个分支之间具有序列关系。实际上，输出层的6个分支的结构一样，并且相互独立，那么这6个分支之间的序列关系是怎么形成的呢？？？

其实输出层中的6个分支之间并没有序列关系，它们能够识别出图片中不同位置的字符是与输出层的权重有关的。在训练时，每个输出层的分支都可以被视为独立的分类网络，该网络根据样本中的标签调整权重，最终使得该输出层中的分支能够识别出样本中的指定标签。当训练多个独立分支时，令输入标签是有顺序的，这样输出的结果也就有顺序了。



8、可视化分类器的关注区域：

输出层的每个分类器中都会执行一个卷积运算，该卷积层的作用就是从完整的图像信息中找出指定区域的图像。可以利用Grad-CAM方法编写可视化代码，可视化输出层中的卷积运算，以便更清晰地显示出模型的关注区域。

计算影像图片的病灶区域属于模型的可视化部分，即找出模型，判断该病灶，并将其显示在影像上。

Grad-CAM方法是乔治亚理工学院等研究单位在2017年提出的一种基于梯度定位的深度网络可视化方法。Grad-CAM以热力图形式解释深度神经网络模型的分类依据，也就是通过图片中的某些像素进行类别判断。

Grad-CAM是一个基于梯度定位的深度网络可视化解释方法。
Gard-CAM的基本原理是，计算最后一个卷积层中每个特征图对于每个类别的权重，然后对每个特征图求加权和，最后把加权后的特征图映射到原始图片上。

Grad-CAM的一个优势就是不必重新训练网络，便于用在可视化图片分类任务中，详细情况可参阅论文："Grad-CAM：Visual Explanations from Deep Networks via Gradient-based Localization"。

感触：这个东西相当有意思，相当于可以给出神经网络黑箱一种统计学解释，如果以后我们碰到过拟合问题，或无法训练好的问题，可以直接从热力图看结果，看神经网络关注的主要像素区域到底对不对，和人眼人脑关注的有无差异。如果不对，一定说明模型有问题，很可能出现误判问题！！！


9、我做的这个创新真的有意义吗？？？你看车牌号就是7个字符，那我还用什么crnn呢，干脆直接利用7个卷积网络分别识别不就解决了吗？？？其实这么做也不失为一种好办法，但有种实际情况，车牌号未必就是固定7个字符，还有摩托车、电动车呢，我这个算法就相当于把所有车牌号都可以一起识别掉。另外有些情况字符序列可大可小，我这个创新还是能拓宽适用范围的。还有，crnn后面可以接入stn矫正，如果你对每个区域分别识别，矫正信息就不统一。



10、string模块主要包含关于字符串的处理函数：

# 常用字符串常量：
# string.digits：包含数字0~9的字符串
# string.lowercase：包含所有小写字母的字符串
# string.uppercase：包含所有大写字母的字符串
# string.letters：包含所有字母(大写或小写字符串，在python3.0中，使用string.ascii-letters代替)
# string.printable：包含所有可打印字符的字符串
# string.punctuation：包含所有标点的字符串


generate_image()方法接收一个字符串参数，将生成次字符串内容的验证码，返回的是PIL模块中的Image对象。
ImageCaptcha(width=160, height=60, fonts=None, font_sizes=None) 类实例化时，还可传入四个参数:

# width: 生成验证码图片的宽度，默认为160个像素；
# height： 生成验证码图片的高度，默认为60个像素；
# fonts： 字体文件路径，用于生成验证码时的字体，默认使用模块自带DroidSansMono.ttf字体，你可以将字体文件传入,生成验证码时将随机使用;
# font_sizes： 控制验证码字体大小，同fonts一样，接收一个list或者tuple,随机使用



11、训练过程中最奇怪的事情就是......

我在本机上取adam学习率1e-3，最后瓶颈时分类精度居然能达到94%，但我在服务器上取adam学习率1e-4，最后瓶颈时分类精度却只能达到82%左右？？？到底是哪里出现了问题，居然差异这么大？？？

讲道理就算学习率设置的没有特别好，性能差异也不会如此巨大吧？？？

一方面是学习率设置不同，另一方面是因为我测试集取错了，不小心包括了很多训练集的数据，所以效果才这么良好......经过debug之后，训练达到瓶颈的情况下，6个字符整个序列识别准确率只有25%左右，但单个字符的识别准确率能达到75%。


12、哪怕重新生成的单纯只含字母的数据集，多尺度CNN的OCR序列识别效果依然不佳，精度只能达到30%左右。

主要是两个方面限制住了精度：
（1）随机生成的字符间距差异太大，分割位置难以把握，有的间距很长有的间距很短。
（2）字符形变扭曲过于严重，真的有时候连人眼都无法识别准确。


最后我降低了模拟数据的复杂程度，只考虑使用数字0-9这十种类别，现在算法效果好多了，主要是扭曲之后依然比较好区分，现在模型的序列识别精度可以达到65%。
